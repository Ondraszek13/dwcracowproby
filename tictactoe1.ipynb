{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tictactoe1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nwt0E5knhiq7YORwlkqpCNRVP2esucCd",
      "authorship_tag": "ABX9TyOnCk1bUUc+U0LK0WxfFpxC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFHPjLFmGW5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A Python Program to implement Machine Learning for the Game Tic Tac Toe (3x3) using Reinforcement Learning (Q learning technique) and tensorflow. \n",
        "# \n",
        "#   Note: mistakes were made, especially spelling mistakes.\n",
        "#   \n",
        "#\n",
        "\n",
        "\n",
        "# imports\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Varibles \n",
        "game_rows = rows = 3\n",
        "game_cols = cols = 3\n",
        "winning_length = 3\n",
        "boardSize = rows * cols\n",
        "actions = rows * cols\n",
        "won_games = 0\n",
        "lost_games = 0\n",
        "draw_games = 0\n",
        "layer_1_w = 750\n",
        "layer_2_w = 750\n",
        "layer_3_w = 750\n",
        "\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.random.truncated_normal(shape, stddev = 0.01) ##random\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.01, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "# greedy policy for selecting an action\n",
        "# the higher the value of e the higher the probability of an action being random.\n",
        "epsilon = 1.0\n",
        "\n",
        "# Discount factor -- determines the importance of future rewards\n",
        "GAMMA = 0.9\n",
        "\n",
        "\n",
        "# swaps X's to O's and vice versa\n",
        "def InverseBoard(board):\n",
        "    temp_board = np.copy(board)\n",
        "    rows, cols = temp_board.shape\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            temp_board[r,c] *= -1\n",
        "    return temp_board.reshape([-1])\n",
        "\n",
        "# returns true if the game is completed for a given board\n",
        "def isGameOver(board):\n",
        "    temp = None\n",
        "    rows , cols = board.shape\n",
        "\n",
        "    ## ROWS\n",
        "    for i in range(rows):\n",
        "        temp = getRowSum(board, i)\n",
        "        if checkValue(temp):\n",
        "            return True\n",
        "    ## COLS\n",
        "    for i in range(cols):\n",
        "        temp = getColSum(board, i)\n",
        "        if checkValue(temp):\n",
        "            return True\n",
        "\n",
        "    ## Diagonals\n",
        "    temp = getRightDig(board)\n",
        "    if checkValue(temp):\n",
        "        return True\n",
        "\n",
        "    temp = getLeftDig(board)\n",
        "    if checkValue(temp):\n",
        "        return True\n",
        "\n",
        "    # ## Does not contain empty places\n",
        "    # empty_place_exist = False\n",
        "    # for r in range(rows):\n",
        "    #     for c in range(cols):\n",
        "    #         if(board[r,c] == 0):\n",
        "    #             empty_place_exist = True\n",
        "    # if not empty_place_exist:\n",
        "    #     return True\n",
        "\n",
        "    return False\n",
        "\n",
        "## support function\n",
        "def getRowSum(board , r):\n",
        "    rows , cols = board.shape\n",
        "    sum = 0\n",
        "    for c in range(cols):\n",
        "        sum = sum + board[r,c]\n",
        "    return sum\n",
        "\n",
        "## support function\n",
        "def getColSum(board , c):\n",
        "    rows , cols = board.shape\n",
        "    sum = 0\n",
        "    for r in range(rows):\n",
        "        sum = sum + board[r,c]\n",
        "    return sum\n",
        "\n",
        "## support function\n",
        "def getLeftDig(board):\n",
        "    rows , cols = board.shape\n",
        "    sum = 0\n",
        "    for i in range(rows):\n",
        "        sum = sum + board[i,i]\n",
        "    return sum\n",
        "\n",
        "## support function\n",
        "def getRightDig(board):\n",
        "    rows , cols = board.shape\n",
        "    sum = 0\n",
        "    i = rows - 1\n",
        "    j = 0\n",
        "    while i >= 0:\n",
        "        sum += board[i,j]\n",
        "        i = i - 1\n",
        "        j = j + 1\n",
        "    return sum\n",
        "\n",
        "## support function\n",
        "def checkValue(sum):\n",
        "    if sum == -3 or sum == 3:\n",
        "        return True\n",
        "\n",
        "\n",
        "# creates the network\n",
        "def createNetwork():\n",
        "    # network weights and biases\n",
        "\n",
        "    W_layer1 = weight_variable([boardSize, layer_1_w])\n",
        "    b_layer1 = bias_variable([layer_1_w])\n",
        "\n",
        "    W_layer2 = weight_variable([layer_1_w, layer_2_w])\n",
        "    b_layer2 = bias_variable([layer_2_w])\n",
        "\n",
        "    W_layer3 = weight_variable([layer_2_w, layer_3_w])\n",
        "    b_layer3 = bias_variable([layer_3_w])\n",
        "\n",
        "    o_layer = weight_variable([layer_3_w, actions])\n",
        "    o_bais  = bias_variable([actions])\n",
        "\n",
        "    # input Layer\n",
        "    x = tf.compat.v1.placeholder(\"float\", [None, boardSize]) ## tf.placeholder\n",
        "\n",
        "    # hidden layers\n",
        "    h_layer1 = tf.nn.relu(tf.matmul(x,W_layer1) + b_layer1)\n",
        "    h_layer2 = tf.nn.relu(tf.matmul(h_layer1,W_layer2) + b_layer2)\n",
        "    h_layer3 = tf.nn.relu(tf.matmul(h_layer2,W_layer3) + b_layer3)\n",
        "\n",
        "    # output layer\n",
        "    y = tf.matmul(h_layer3,o_layer) + o_bais\n",
        "    prediction = tf.argmax(y[0])\n",
        "\n",
        "    return x,y, prediction\n",
        "\n",
        "\n",
        "def tainNetwork():\n",
        "    print()\n",
        "\n",
        "    # create network\n",
        "    inputState , Qoutputs, prediction = createNetwork()\n",
        "\n",
        "    # calculate the loss\n",
        "    targetQOutputs = tf.compat.v1.placeholder(\"float\",[None,actions])   ## tf.placeholder\n",
        "    loss =  tf.reduce_mean(tf.square(tf.subtract(targetQOutputs, Qoutputs)))\n",
        "\n",
        "    # train the model to minimise the loss\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "\n",
        "    # creating a sesion\n",
        "    sess = tf.InteractiveSession()\n",
        "\n",
        "    # saving and loading networks\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # load a saved model\n",
        "    step = 0\n",
        "    iterations = 0\n",
        "\n",
        "    checkpoint = tf.train.get_checkpoint_state(\"model\")\n",
        "    if checkpoint and checkpoint.model_checkpoint_path:\n",
        "        s = saver.restore(sess,checkpoint.model_checkpoint_path)\n",
        "        print(\"Successfully loaded the model:\", checkpoint.model_checkpoint_path)\n",
        "        step = int(os.path.basename(checkpoint.model_checkpoint_path).split('-')[1])\n",
        "    else:\n",
        "        print(\"Could not find old network weights\")\n",
        "    iterations += step\n",
        "\n",
        "    print(time.ctime())\n",
        "\n",
        "    ## define maximum number of matches for inital interation\n",
        "    tot_matches = 60000\n",
        "    number_of_matches_each_episode = 500\n",
        "    max_iterations = tot_matches / number_of_matches_each_episode\n",
        "    \n",
        "    # defines the rate at which epsilon should decrease\n",
        "    e_downrate = 0.9 / max_iterations\n",
        "\n",
        "    print(\"e down rate is \",e_downrate)\n",
        "\n",
        "    # initalise e with inital epsilon value here.\n",
        "    e = epsilon\n",
        "\n",
        "    print(\"max iteration = {}\".format(max_iterations))\n",
        "    print()\n",
        "    \n",
        "    run_time = 0\n",
        "    while \"ticky\" != \"tacky\":\n",
        "        sys.stdout.flush()\n",
        "        start_time = time.time()\n",
        "        episodes = number_of_matches_each_episode\n",
        "        global won_games\n",
        "        global lost_games\n",
        "        global draw_games\n",
        "\n",
        "        # sum of the losses while training the model.\n",
        "        total_loss = 0\n",
        "\n",
        "        epchos = 100\n",
        "        GamesList = []\n",
        "\n",
        "        for i in range(episodes):\n",
        "            completeGame, victory = playaGame(e,sess,inputState, prediction,Qoutputs)\n",
        "            GamesList.append(completeGame)\n",
        "            \n",
        "\n",
        "        for k in range(epchos):\n",
        "            random.shuffle(GamesList)\n",
        "            for i in GamesList:\n",
        "                len_complete_game = len(i)\n",
        "                loop_in = 0\n",
        "                game_reward = 0\n",
        "                while loop_in < len_complete_game:\n",
        "                    j = i.pop()\n",
        "                    currentState = j[0]\n",
        "                    action = j[1][0]\n",
        "                    reward = j[2][0]\n",
        "                    nextState = j[3]\n",
        "\n",
        "                    ## Game end reward\n",
        "                    if loop_in == 0:\n",
        "                        game_reward = reward\n",
        "                    else:\n",
        "                        #obtain q values for next sate using the network\n",
        "                        nextQ = sess.run(Qoutputs,feed_dict={inputState:[nextState]})\n",
        "                        maxNextQ = np.max(nextQ)\n",
        "                        game_reward = GAMMA * ( maxNextQ )\n",
        "\n",
        "                    \n",
        "                    targetQ = sess.run(Qoutputs,feed_dict={inputState:[currentState]})\n",
        "\n",
        "                    # once we calculate the reward to the paticular action, we must also add the -1 reward for all the illegal moves in the q value\n",
        "                    # one might say this is cheating , but it speeds up the process.\n",
        "                    for index,item in enumerate(currentState):\n",
        "                        if item != 0:\n",
        "                            targetQ[0,index] = -1\n",
        "\n",
        "                    targetQ[0,action] = game_reward\n",
        "\n",
        "                    loop_in += 1\n",
        "                    t_loss = 0\n",
        "                    #Train our network using the targetQ\n",
        "\n",
        "                    \n",
        "                    t_loss=sess.run([train_step,Qoutputs,loss],feed_dict={inputState:[currentState], targetQOutputs:targetQ})\n",
        "                    total_loss += t_loss[2]\n",
        "\n",
        "        iterations += 1\n",
        "        time_diff = time.time()-start_time\n",
        "        run_time += time_diff\n",
        "        print(\"iteration {} completed with {} wins, {} losses {} draws, out of {} games played, e is {} \\ncost is {} , current_time is {}, time taken is {} , total time = {} hours \\n\".format(iterations,\n",
        "        won_games,lost_games,draw_games,episodes,e*100,total_loss,time.ctime(),time_diff,(run_time)/3600))\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "        won_games = 0\n",
        "        lost_games = 0\n",
        "        draw_games = 0\n",
        "\n",
        "        # decrease e value slowly.\n",
        "        if e > -0.2:\n",
        "            e -= e_downrate\n",
        "        else:\n",
        "             e = random.choice([0.1,0.05,0.06,0.07,0.15,0.03,0.20,0.25,0.5,0.4])\n",
        "\n",
        "        #print(wins,loss,(episodes-wins-loss))\n",
        "        saver.save(sess, \"./model/model.ckpt\",global_step=iterations)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plays a game and returns a list with all states, actions and final reward.\n",
        "def playaGame(e,sess,inputState, prediction, Qoutputs):\n",
        "    global won_games\n",
        "    global lost_games\n",
        "    global draw_games\n",
        "\n",
        "    win_reward = 10\n",
        "    loss_reward = -1\n",
        "    draw_reward = 3\n",
        "\n",
        "    ## create the entire game memory object that contains the memories for the game\n",
        "    ## and an empty board\n",
        "    completeGameMemory = []\n",
        "    myList = np.array([0]*(rows*cols)).reshape(3,3)\n",
        "\n",
        "    ## randomly chose a turn 1 is ours -1 is oppnents\n",
        "    turn = random.choice([1,-1])\n",
        "\n",
        "    ## if opponents turn let him play and set the inital state\n",
        "    if(turn == -1):\n",
        "        initial_index = random.choice(range(9))\n",
        "        best_index, _= sess.run([prediction,Qoutputs], feed_dict={inputState : [np.array(np.copy(myList).reshape(-1))]})\n",
        "        initial_index = random.choice([best_index,initial_index,best_index])\n",
        "        myList[int(initial_index/3),initial_index%3] = -1\n",
        "        turn = turn * -1\n",
        "\n",
        "    ## while the game is not over repat, our move then opponents move\n",
        "    while(True):\n",
        "\n",
        "        ## create a memory which will hold the current inital state, the action thats taken, the reward the was recieved, the next state\n",
        "        memory = []\n",
        "\n",
        "        ## create a copy of the board which is linear\n",
        "        temp_copy = np.array(np.copy(myList).reshape(-1))\n",
        "\n",
        "        ## fetch all the indexes that are free or zero so those can used for playing next move\n",
        "        zero_indexes = []\n",
        "        for index,item in enumerate(temp_copy):\n",
        "            if item == 0:\n",
        "                zero_indexes.append(index)\n",
        "\n",
        "        ## if no index is found which is free to place a move exit as the game completed with slight reward. better to draw then to lose right ?\n",
        "        if len(zero_indexes) == 0:\n",
        "            reward = draw_reward\n",
        "            completeGameMemory[-1][2][0] = reward\n",
        "            draw_games += 1\n",
        "            break\n",
        "\n",
        "        ## if free indexs are found randomly select one which will be later can be used as the action.\n",
        "        selectedRandomIndex = random.choice(zero_indexes)\n",
        "\n",
        "        ## calculate the prediction from the network which can be later used as an action with some probability\n",
        "        pred, _ = sess.run([prediction,Qoutputs], feed_dict={inputState : [temp_copy]})\n",
        "\n",
        "        ## since the netowrk can be messy and inacurate check if the prediction is correct first.\n",
        "        isFalsePrediction = False if temp_copy[pred] == 0 else True\n",
        "\n",
        "        ## lets add the inital state to the current memory\n",
        "        memory.append(np.copy(myList).reshape(-1))\n",
        "\n",
        "        ## Lets pick an action with some probability, exploration and exploitation\n",
        "        if random.random() > e: #and isFalsePrediction == False: #expliotation\n",
        "            action = pred\n",
        "        else: # exploration, explore with valid moves to save time.\n",
        "            random_action = random.choice(range(9))\n",
        "            action = selectedRandomIndex\n",
        "            #action = random.choice([selectedRandomIndex,random_action])\n",
        "            #action = random.choice(range(9))\n",
        "\n",
        "        ## lets add the action to the memory\n",
        "        memory.append([action])\n",
        "\n",
        "        ## randomly plays a wrong move.. unlucky, however.\n",
        "        if action not in zero_indexes:\n",
        "            reward = loss_reward\n",
        "            memory.append([reward])\n",
        "            memory.append(np.copy(myList.reshape(-1)))\n",
        "            completeGameMemory.append(memory)\n",
        "            lost_games +=1\n",
        "            break\n",
        "\n",
        "        ## update the board with the action taken\n",
        "        myList[int(action/game_rows),action%game_cols] = 1\n",
        "\n",
        "        ## now calcualte the reward.\n",
        "        reward = 0\n",
        "\n",
        "        ## if we choose an action thats invalid, boo we get no reward and opponent wins\n",
        "        if isFalsePrediction == True and action == pred:\n",
        "            reward = loss_reward\n",
        "            memory.append([reward])\n",
        "            memory.append(np.copy(myList.reshape(-1)))\n",
        "            completeGameMemory.append(memory)\n",
        "            lost_games +=1\n",
        "            break\n",
        "\n",
        "        ## if after playing our move the game is completed then yay we deserve a reward and its the final state\n",
        "        if(isGameOver(myList)):\n",
        "            reward = win_reward\n",
        "            memory.append([reward])\n",
        "            memory.append(np.copy(myList.reshape(-1)))\n",
        "            completeGameMemory.append(memory)\n",
        "            won_games +=1\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        # Now lets make a move for the opponent\n",
        "\n",
        "        ## same as before, but since we are finding a move for the opponent we use the inverse board\n",
        "        ## to calculate the prediction\n",
        "        temp_copy_inverse = np.array(np.copy(InverseBoard(myList)).reshape(-1))\n",
        "        temp_copy = np.array(np.copy(myList).reshape(-1))\n",
        "        zero_indexes = []\n",
        "        for index,item in enumerate(temp_copy):\n",
        "            if item == 0:\n",
        "                zero_indexes.append(index)\n",
        "\n",
        "        ## if opponent has no moves left that means that the last move was the final move and its a draw so some reward\n",
        "        if len(zero_indexes) == 0:\n",
        "            reward = draw_reward\n",
        "            memory.append([reward])\n",
        "            memory.append(np.copy(myList.reshape(-1)))\n",
        "            completeGameMemory.append(memory)\n",
        "            draw_games+=1\n",
        "            break\n",
        "\n",
        "        ## almost same as before\n",
        "        selectedRandomIndex = random.choice(zero_indexes)\n",
        "        pred, _ = sess.run([prediction,Qoutputs], feed_dict={inputState : [temp_copy_inverse]})\n",
        "        isFalsePrediction = False if temp_copy[pred] == 0 else True\n",
        "\n",
        "        ## we want opponet to play good sometimes and play bad sometimes so 33.33% ish probability\n",
        "        action = None\n",
        "\n",
        "        if(isFalsePrediction == True):\n",
        "            action = random.choice([selectedRandomIndex])\n",
        "        else:\n",
        "            action = random.choice([selectedRandomIndex,pred,pred,pred,pred])\n",
        "            #action = random.choice([selectedRandomIndex,pred])\n",
        "        # if e < 0.4 and isFalsePrediction == False:\n",
        "        #     action = pred\n",
        "\n",
        "        # testing\n",
        "        temp_copy2 = np.copy(myList).reshape(-1)\n",
        "        if temp_copy2[action] != 0:\n",
        "            print(\"big time error here \",temp_copy2 , action)\n",
        "            return\n",
        "\n",
        "        ## update the board with opponents move\n",
        "        myList[int(action/game_rows),action%game_cols] = -1\n",
        "\n",
        "        ## if after opponents move the game is done meaning opponent won, boo..\n",
        "        if isGameOver(myList) == True:\n",
        "            reward = loss_reward\n",
        "            memory.append([reward])\n",
        "            #final state\n",
        "            memory.append(np.copy(myList.reshape(-1)))\n",
        "            completeGameMemory.append(memory)\n",
        "            lost_games +=1\n",
        "            break\n",
        "\n",
        "        ## if no one won and game isn't done yet then lets continue the game\n",
        "        memory.append([0])\n",
        "        memory.append(np.copy(myList.reshape(-1)))\n",
        "\n",
        "        ## lets add this move to the complete game memory\n",
        "        completeGameMemory.append(memory)\n",
        "\n",
        "    # return the complete game memory and the last set reward\n",
        "    return completeGameMemory,reward\n",
        "\n",
        "\n",
        "# not used.\n",
        "class Logger(object):\n",
        "    def __init__(self):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(\"output.log\", \"w\")\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "\n",
        "    def flush(self):\n",
        "        #this flush method is needed for python 3 compatibility.\n",
        "        #this handles the flush command by doing nothing.\n",
        "        #you might want to specify some extra behavior here.\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #sys.stdout = Logger()\n",
        "    tainNetwork()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3GIdWD7Gd8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}